---
title: "p8105_hw6_hq2163_hanbo"
author: "Hanbo Qiu"
date: "November 19, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
  )
  
library(tidyverse)
library(ggridges)
library(modelr)
library(mgcv)
theme_set(theme_bw() + theme(plot.title = element_text(hjust = 0.5)))
```

##Problem 1

**Import a dataset and make some alternation on some variables:**

```{r message=FALSE, warning=FALSE}
homicides = read_csv(file = "./homicide-data.csv") %>% 
   mutate(city_state = str_c(city, ", ", state),
        resolved = as.numeric(disposition == "Closed by arrest"),
        victim_age = as.numeric(victim_age),
        victim_race = ifelse(victim_race == "White", "white", "non-white"), 
        victim_race = relevel(factor(victim_race),ref = "white")) %>% 
   filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))
```

Create a `city_state` variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved. Omit cities `Dallas, TX`, `Phoenix, AZ`, `Kansas City, MO` and `Tulsa, AL`. Modifiy `victim_race` to have categories white and  non-white, with white as the reference category. Modify `victim_age` as numeric.

**Fit the model of "Baltimore, MD":**

```{r}
baltimore_df = homicides %>% 
  filter(city_state == "Baltimore, MD" )

fit_logistic = 
  glm(resolved ~ victim_age + victim_sex + victim_race, 
    data = baltimore_df, 
    family = binomial()) 
```

We fitted a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors for Baltimore, MD.

**Calculate the OR:**

```{r}
ci = fit_logistic %>% 
  confint() %>%
  as_tibble() %>% 
  rename("ci_low" = "2.5 %", "ci_high" = "97.5 %") %>% 
  mutate(ci_low = exp(ci_low), ci_high = exp(ci_high))

fit_logistic %>% 
  broom::tidy() %>% 
  mutate(city_state = "Baltimore, MD", OR = exp(estimate)) %>% 
  select(city_state, OR) %>% 
  bind_cols(ci) %>% 
  tail(1) %>% 
  knitr::kable(digits = 3)
```

The above table shows that the estimate odds ratio and CI for solving homicides comparing non-white victims to white victims keeping all other variables fixed.

**Fit the model each of the cities :**

```{r message=FALSE, warning=FALSE}
nest_lm_res =
  homicides %>% 
  group_by(city_state) %>% 
  nest() %>% 
  mutate(models = map(data, ~glm(resolved ~ victim_age + victim_sex + victim_race, data = .x, family = binomial())),
         ci = map(.x=models, ~confint(.x)),
         or = map(models, broom::tidy),
         ci = map(ci, broom::tidy)) %>% 
  select(-data,-models) %>% 
  unnest() %>% 
  filter(term == "victim_racenon-white") %>% 
  select(city_state, or = estimate, ci_low = "X2.5..", ci_high = "X97.5..") %>% 
  mutate(or = exp(or), ci_low = exp(ci_low), ci_high = exp(ci_high))
```

**Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot:**

```{r}
nest_lm_res %>% 
  mutate(city_state = fct_reorder(city_state, or)) %>% 
  ggplot(aes(x = city_state, y = or)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = ci_low, ymax = ci_high)) + 
  labs(
    title = "The estimated ORs and CIs for each city",
    x = "City",
    y = "The estimated ORs and CIs",
    caption = "Data from The Washington Post"
  ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This figure suggests a very wide range in odds ratio (and CI) for solving homicides comparing non-white victims to white victims. `Tampa, FL` is noticeably high and, given the width of the CI, meaning that for every increasing non-white victims comparing to white victims, the OR for solving homicides is higest. 

##Problem 2

**Import a dataset and make some alternation on some variables:**

```{r message=FALSE, warning=FALSE}
birth_wt = read_csv(file = "./birthweight.csv") %>% 
  mutate(babysex = factor(babysex, labels = c("Male", "Female")),
         frace = factor(frace, levels = c(1, 2 ,3 ,4 ,8 ,9),labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
         mrace = factor(mrace, levels = c(1, 2 ,3 ,4 ,8),labels = c("White", "Black", "Asian", "Puerto Rican", "Other")),
         malform = factor(malform, labels = c("absent", "present")),
         fincome = fincome * 100) %>% 
  select(bwt, everything())
```

Convert numeric variables `babysex`, `frace`, `mrace` and `malform` to factor variables.

```{r}
apply(birth_wt, 2, function(x){sum(is.na(x))})
```

The above table shows there is no missing value for all variables.

**Use Stepwise regreession methods to build model.**                                       
```{r}
mult.fit = lm(bwt ~ ., data=birth_wt)
step(mult.fit, direction='backward')
```

`step` function uses AIC criterion for variable selection and the default option is `backward`. As a result, it reported a reasonable linear model with variables of `babysex`, `bhead`, `blength`, `delwt`, `fincome`, `gaweeks`, `mheight`, `mrace`, `parity`, `ppwt` and `smoken`.                                  

**show a plot of model residuals against fitted values**  

```{r}
fit_1 = lm(formula = bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = birth_wt)

birth_wt%>% 
  modelr::add_predictions(fit_1) %>% 
  modelr::add_residuals(fit_1) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_hex() +
  geom_hline(aes(yintercept=0), colour="red")
```

**Create different models and compare these models in terms of the cross-validated prediction error**  

```{r message=FALSE, warning=FALSE}
cv_df = 
  crossv_mc(birth_wt, 100) %>% 
  mutate(train = map(train, as_tibble),
         teat = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(lin_1    = map(train, ~lm(formula = bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
         lin_2    = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         nonlin_2 = map(train, ~gam(bwt ~ s(blength) + s(gaweeks), data = .x)),
         lin_3    = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x)),
         nonlin_3 = map(train, ~gam(bwt ~ te(bhead, blength, by = babysex), data = .x))) %>% 
  mutate(rmse_lin_1    = map2_dbl(lin_1, test, ~rmse(model = .x, data = .y)),
         rmse_lin_2    = map2_dbl(lin_2 , test, ~rmse(model = .x, data = .y)),
         rmse_nonlin_2 = map2_dbl(nonlin_2, test, ~rmse(model = .x, data = .y)),
         rmse_lin_3    = map2_dbl(lin_3 , test, ~rmse(model = .x, data = .y)),
         rmse_nonlin_3 = map2_dbl(nonlin_3 , test, ~rmse(model = .x, data = .y)))
```

`lin_1` is my model. `lin_2` and `nonlin_2` are linear and non-linear model using length at birth and gestational age as predictors. `lin_3` and `nonlin_3` are linear and non-linear model using head circumference, length, sex, and all interactions (including the three-way interaction) between these

**Make plot to compare the `RMSEs` of each model**  

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

When we applied the cross-validated prediction error to compare these models, we found that model lin_1 which was built based on the Stepwise regreession methods have the least residuls, meaning fitting the best. model 2 is the one using length at birth and gestational age as predictors. We used both linear and nonlinear to fit it. Both of them have large residuals.The model using head circumference, length, sex, and all interactions (including the three-way interaction) seems to have acceptable risiduals, but the CI of risidual is too wide which means a large deviation of predication.