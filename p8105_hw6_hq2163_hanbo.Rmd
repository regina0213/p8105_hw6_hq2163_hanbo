---
title: "p8105_hw6_hq2163_hanbo"
author: "Hanbo Qiu"
date: "November 19, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
  )
  
library(tidyverse)
library(ggridges)
library(modelr)
library(mgcv)
theme_set(theme_bw() + theme(plot.title = element_text(hjust = 0.5)))
```

##Problem 1

**Import a dataset and make some alternation on some variables:**

```{r}
homicides = read_csv(file = "./homicide-data.csv") %>% 
   mutate(city_state = str_c(city, ", ", state),
        resolved = as.numeric(disposition == "Closed by arrest"),
        victim_age = as.numeric(victim_age),
        victim_race = ifelse(victim_race == "White", "white", "non-white"), 
        victim_race = relevel(factor(victim_race),ref = "white")) %>% 
   filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))
```

Create a city_state variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved. Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO and Tulsa, AL. Modifiy victim_race to have categories white and  non-white, with white as the reference category. Modify victim_age as numeric.

**Fit the model of "Baltimore, MD" :**

```{r}
baltimore_df = homicides %>% 
  filter(city_state == "Baltimore, MD" )
fit_logistic = 
  glm(resolved ~ victim_age + victim_sex + victim_race, 
    data = baltimore_df, 
    family = binomial()) 

  confi_interval=confint(fit_logistic) %>% as.data.frame() %>% 
  tail(1) %>% 
  gather(key = CI, value = value ) %>% 
  mutate(CI_OR = exp(value))
  
  broom::tidy(fit_logistic) %>% 
  mutate(OR = exp(estimate)) %>% 
  tail(1) %>% 
  select(estimate, OR) %>% 
  knitr::kable(digits = 3)
```

We fitted a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors for Baltimore, MD. The estimate for solving homicides comparing non-white victims to white victims keeping all other variables fixed is 0.441. The lower level confidence interval of the adjusted odds ratio of is `r round(confi_interval[[1,3]], digits = 3)`, the upper level confidence interval is `r round(confi_interval[[2,3]], digits = 3)`.

**Fit the model each of the cities :**

```{r message=FALSE, warning=FALSE}
nest_lm_res =
  homicides %>% 
  group_by(city_state) %>% 
  nest() %>% 
  mutate(models = map(data, ~glm(resolved ~ victim_age + victim_sex + victim_race, data = .x, family = binomial())),
         ci = map(.x=models, ~confint(.x)),
         or = map(models, broom::tidy),
         ci = map(ci, broom::tidy)) %>% 
  select(-data,-models) %>% 
  unnest() %>% 
  filter(term == "victim_racenon-white") %>% 
  select(city_state, or = estimate, ci_low = "X2.5..", ci_high = "X97.5..") %>% 
  mutate(or = exp(or), ci_low = exp(ci_low), ci_high = exp(ci_high))
```

**Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot:**

```{r}
nest_lm_res %>% 
  mutate(city_state = fct_reorder(city_state, or)) %>% 
  ggplot(aes(x = city_state, y = or)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = ci_low, ymax = ci_high)) + 
  labs(
    title = "The estimated ORs and CIs for each city",
    x = "City",
    y = "The estimated ORs and CIs",
    caption = "Data from The Washington Post"
  ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This figure suggests a very wide range in odds ratio (and CI) for solving homicides comparing non-white victims to white victims.  -- Tampa, FL is noticeably high and, given the width of the CI, meaning that for every increasing non-white victims comparing to white victims, the OR for solving homicides is higest. 

##Problem 2

**Import a dataset and make some alternation on some variables:**

```{r}
birth_wt = read_csv(file = "./birthweight.csv") %>% 
  mutate(babysex = factor(babysex, labels = c("Male", "Female")),
         frace = factor(frace, levels = c(1, 2 ,3 ,4 ,8 ,9),labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
         mrace = factor(mrace, levels = c(1, 2 ,3 ,4 ,8),labels = c("White", "Black", "Asian", "Puerto Rican", "Other")),
         malform = factor(malform, labels = c("absent", "present")),
         fincome = fincome * 100) %>% 
  select(bwt, everything())

apply(birth_wt, 2, function(x){sum(is.na(x))})
```

Convert some numeric variable to factor. There is no missing value for all variables.

**Use Stepwise regreession methods to build model.**                                       
```{r}
mult.fit <- lm(bwt ~ ., data=birth_wt)
step(mult.fit, direction='backward')
```

'Step' function uses AIC criterion for variable selection and the default option is 'backward'. As a result, it reported a reasonable linear model with variables of babysex, bhead, blength, delwt, fincome, gaweeks, mheight, mrace, parity, ppwt and smoken.                                  

**show a plot of model residuals against fitted values**  

```{r}
fit_1 = lm(formula = bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = birth_wt)

birth_wt%>% 
  modelr::add_predictions(fit_1) %>% 
  modelr::add_residuals(fit_1) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_hex() +
  geom_hline(aes(yintercept=0), colour="red")
```

**Compare different models**  

```{r message=FALSE, warning=FALSE}
cv_df = 
  crossv_mc(birth_wt, 100) %>% 
  mutate(train = map(train, as_tibble),
         teat = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(lin_1    = map(train, ~lm(formula = bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
         lin_2    = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         nonlin_2 = map(train, ~gam(bwt ~ s(blength) + s(gaweeks), data = .x)),
         lin_3    = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x)),
         nonlin_3 = map(train, ~gam(bwt ~ te(bhead, blength, by = babysex), data = .x))) %>% 
  mutate(rmse_lin_1    = map2_dbl(lin_1, test, ~rmse(model = .x, data = .y)),
         rmse_lin_2    = map2_dbl(lin_2 , test, ~rmse(model = .x, data = .y)),
         rmse_nonlin_2 = map2_dbl(nonlin_2, test, ~rmse(model = .x, data = .y)),
         rmse_lin_3    = map2_dbl(lin_3 , test, ~rmse(model = .x, data = .y)),
         rmse_nonlin_3 = map2_dbl(nonlin_3 , test, ~rmse(model = .x, data = .y)))
```

**Compare different models by rmse**  

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

When we applied the cross-validated prediction error to compare these models, we found that model lin_1 which was built based on the Stepwise regreession methods have the leaset residuls,meaning fitting the best. model 2 is the one using length at birth and gestational age as predictors. We used both linear and nonlinear to fit it. Both of them have large residuals.The model using head circumference, length, sex, and all interactions (including the three-way interaction) seems to have acceptable risiduals, but the CI of risidual is too wide which means a large deviation of predication.
